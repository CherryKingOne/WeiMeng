# LangChain + LangGraph Agent Teams æ¶æ„è®¾è®¡

åŸºäºä½ çš„é¡¹ç›®ç»“æ„ï¼Œæˆ‘ä¸ºä½ è®¾è®¡ä¸€ä¸ªå®Œæ•´çš„ 7-Agent å›¢é˜Ÿæ¶æ„æ–¹æ¡ˆï¼š

## ğŸ“ æ¨èç›®å½•ç»“æ„

```
WeiMeng/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â”‚   â”œâ”€â”€ agent/                    # Agent æ ¸å¿ƒæ¨¡å—
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ base/                 # Agent åŸºç¡€è®¾æ–½
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ agent_base.py    # Agent åŸºç±»
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ tool_base.py     # Tool åŸºç±»
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ memory.py        # è®°å¿†ç®¡ç†
â”‚   â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ teams/                # Agent å›¢é˜Ÿç¼–æ’
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ graph_builder.py # LangGraph æ„å»ºå™¨
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ team_coordinator.py  # å›¢é˜Ÿåè°ƒå™¨
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ state_manager.py # çŠ¶æ€ç®¡ç†
â”‚   â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ agents/               # 7ä¸ªå…·ä½“ Agent å®ç°
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ storyboard_agent.py      # 1. åˆ†é•œ
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ editing_agent.py         # 2. å‰ªè¾‘
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ scene_design_agent.py    # 3. åœºæ™¯è®¾è®¡
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ director_agent.py        # 4. å¯¼æ¼”
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ screenwriter_agent.py   # 5. ç¼–å‰§
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ art_design_agent.py     # 6. ç¾æœ¯è®¾è®¡
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ character_design_agent.py # 7. è§’è‰²è®¾è®¡
â”‚   â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ tools/                # Agent å·¥å…·é›†
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ storyboard_tools.py   # åˆ†é•œå·¥å…·
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ editing_tools.py     # å‰ªè¾‘å·¥å…·
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ scene_tools.py        # åœºæ™¯è®¾è®¡å·¥å…·
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ script_tools.py       # ç¼–å‰§å·¥å…·
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ image_tools.py        # ç¾æœ¯è®¾è®¡å·¥å…·
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ character_tools.py    # è§’è‰²è®¾è®¡å·¥å…·
â”‚   â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ workflows/            # å·¥ä½œæµå®šä¹‰
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ development_workflow.py
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ research_workflow.py
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ analysis_workflow.py
â”‚   â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ api/                  # Agent API
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ routes.py
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ schemas.py
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ dependencies.py
â”‚   â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ repositories/         # Agent æ•°æ®æŒä¹…åŒ–
â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ task_repository.py
â”‚   â”‚   â”‚   â”‚       â””â”€â”€ result_repository.py
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â””â”€â”€ captcha/                  # å…¶ä»–ä¸šåŠ¡æ¨¡å—
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ shared/
â”‚   â”‚       â”œâ”€â”€ domain/
â”‚   â”‚       â”œâ”€â”€ infrastructure/
â”‚   â”‚       â”‚   â”œâ”€â”€ llm/                  # LLM æä¾›å•†å°è£… â­
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ base.py           # LLM åŸºç±»æ¥å£
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ factory.py        # LLM å·¥å‚
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ providers/        # å„ä¾›åº”å•†å®ç°
â”‚   â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ openai_provider.py
â”‚   â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ anthropic_provider.py
â”‚   â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ azure_provider.py
â”‚   â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ google_provider.py
â”‚   â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ deepseek_provider.py
â”‚   â”‚       â”‚   â”‚   â”‚   â””â”€â”€ local_provider.py  # Ollama/vLLM
â”‚   â”‚       â”‚   â”‚   â”‚
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ embeddings/       # Embedding æ¨¡å‹
â”‚   â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚       â”‚   â”‚   â”‚   â””â”€â”€ providers/
â”‚   â”‚       â”‚   â”‚   â”‚
â”‚   â”‚       â”‚   â”‚   â””â”€â”€ config.py         # LLM é…ç½®
â”‚   â”‚       â”‚   â”‚
â”‚   â”‚       â”‚   â”œâ”€â”€ vector_store/        # å‘é‡æ•°æ®åº“
â”‚   â”‚       â”‚   â”œâ”€â”€ cache/               # ç¼“å­˜å±‚
â”‚   â”‚       â”‚   â””â”€â”€ monitoring/          # ç›‘æ§å’Œæ—¥å¿—
â”‚   â”‚       â”‚
â”‚   â”‚       â”œâ”€â”€ security/
â”‚   â”‚       â””â”€â”€ middleware/
â”‚   â”‚
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ settings.py
â”‚   â”‚   â”œâ”€â”€ ai.py                        # AI é…ç½®
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚
â”‚   â””â”€â”€ tests/
â”‚       â”œâ”€â”€ unit/
â”‚       â”‚   â”œâ”€â”€ agent/
â”‚       â”‚   â””â”€â”€ llm/
â”‚       â””â”€â”€ integration/
â”‚           â””â”€â”€ agent_teams/
```

---

## ğŸ—ï¸ æ ¸å¿ƒå®ç°ä»£ç 

### 1. LLM æä¾›å•†åŸºç¡€æ¶æ„

#### `shared/infrastructure/llm/base.py`

```python
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional, AsyncIterator
from dataclasses import dataclass
from enum import Enum

class LLMProvider(str, Enum):
    """æ”¯æŒçš„ LLM æä¾›å•†"""
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    AZURE = "azure"
    GOOGLE = "google"
    DEEPSEEK = "deepseek"
    LOCAL = "local"  # Ollama/vLLM

@dataclass
class LLMConfig:
    """LLM é…ç½®"""
    provider: LLMProvider
    model_name: str
    api_key: Optional[str] = None
    api_base: Optional[str] = None
    temperature: float = 0.7
    max_tokens: int = 2000
    timeout: int = 60
    streaming: bool = False
    extra_params: Dict[str, Any] = None

@dataclass
class LLMResponse:
    """LLM å“åº”"""
    content: str
    model: str
    usage: Dict[str, int]
    metadata: Dict[str, Any] = None

class BaseLLMProvider(ABC):
    """LLM æä¾›å•†åŸºç±»"""
  
    def __init__(self, config: LLMConfig):
        self.config = config
        self._client = None
  
    @abstractmethod
    async def initialize(self):
        """åˆå§‹åŒ–å®¢æˆ·ç«¯"""
        pass
  
    @abstractmethod
    async def generate(
        self, 
        messages: List[Dict[str, str]], 
        **kwargs
    ) -> LLMResponse:
        """ç”Ÿæˆå“åº”"""
        pass
  
    @abstractmethod
    async def stream_generate(
        self, 
        messages: List[Dict[str, str]], 
        **kwargs
    ) -> AsyncIterator[str]:
        """æµå¼ç”Ÿæˆ"""
        pass
  
    @abstractmethod
    async def close(self):
        """å…³é—­è¿æ¥"""
        pass
  
    def get_langchain_llm(self):
        """è¿”å› LangChain LLM å®ä¾‹"""
        pass
```

#### `shared/infrastructure/llm/factory.py`

```python
from typing import Dict, Type
from .base import BaseLLMProvider, LLMProvider, LLMConfig
from .providers.openai_provider import OpenAIProvider
from .providers.anthropic_provider import AnthropicProvider
from .providers.azure_provider import AzureProvider
from .providers.google_provider import GoogleProvider
from .providers.deepseek_provider import DeepSeekProvider
from .providers.local_provider import LocalProvider

class LLMFactory:
    """LLM æä¾›å•†å·¥å‚"""
  
    _providers: Dict[LLMProvider, Type[BaseLLMProvider]] = {
        LLMProvider.OPENAI: OpenAIProvider,
        LLMProvider.ANTHROPIC: AnthropicProvider,
        LLMProvider.AZURE: AzureProvider,
        LLMProvider.GOOGLE: GoogleProvider,
        LLMProvider.DEEPSEEK: DeepSeekProvider,
        LLMProvider.LOCAL: LocalProvider,
    }
  
    @classmethod
    def create(cls, config: LLMConfig) -> BaseLLMProvider:
        """åˆ›å»º LLM æä¾›å•†å®ä¾‹"""
        provider_class = cls._providers.get(config.provider)
        if not provider_class:
            raise ValueError(f"Unsupported provider: {config.provider}")
      
        return provider_class(config)
  
    @classmethod
    def register_provider(
        cls, 
        provider: LLMProvider, 
        provider_class: Type[BaseLLMProvider]
    ):
        """æ³¨å†Œæ–°çš„æä¾›å•†"""
        cls._providers[provider] = provider_class
```

#### `shared/infrastructure/llm/providers/openai_provider.py`

```python
from typing import List, Dict, AsyncIterator
from openai import AsyncOpenAI
from langchain_openai import ChatOpenAI

from ..base import BaseLLMProvider, LLMResponse

class OpenAIProvider(BaseLLMProvider):
    """OpenAI æä¾›å•†å®ç°"""
  
    async def initialize(self):
        """åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯"""
        self._client = AsyncOpenAI(
            api_key=self.config.api_key,
            base_url=self.config.api_base,
            timeout=self.config.timeout
        )
  
    async def generate(
        self, 
        messages: List[Dict[str, str]], 
        **kwargs
    ) -> LLMResponse:
        """ç”Ÿæˆå“åº”"""
        if not self._client:
            await self.initialize()
      
        response = await self._client.chat.completions.create(
            model=self.config.model_name,
            messages=messages,
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens,
            **kwargs
        )
      
        return LLMResponse(
            content=response.choices[0].message.content,
            model=response.model,
            usage={
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens,
            }
        )
  
    async def stream_generate(
        self, 
        messages: List[Dict[str, str]], 
        **kwargs
    ) -> AsyncIterator[str]:
        """æµå¼ç”Ÿæˆ"""
        if not self._client:
            await self.initialize()
      
        stream = await self._client.chat.completions.create(
            model=self.config.model_name,
            messages=messages,
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens,
            stream=True,
            **kwargs
        )
      
        async for chunk in stream:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
  
    def get_langchain_llm(self):
        """è¿”å› LangChain LLM å®ä¾‹"""
        return ChatOpenAI(
            model=self.config.model_name,
            api_key=self.config.api_key,
            base_url=self.config.api_base,
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens,
        )
  
    async def close(self):
        """å…³é—­è¿æ¥"""
        if self._client:
            await self._client.close()
```

#### `shared/infrastructure/llm/providers/deepseek_provider.py`

```python
from typing import List, Dict, AsyncIterator
from openai import AsyncOpenAI
from langchain_openai import ChatOpenAI

from ..base import BaseLLMProvider, LLMResponse

class DeepSeekProvider(BaseLLMProvider):
    """DeepSeek æä¾›å•†å®ç°"""
  
    async def initialize(self):
        """åˆå§‹åŒ– DeepSeek å®¢æˆ·ç«¯"""
        self._client = AsyncOpenAI(
            api_key=self.config.api_key,
            base_url=self.config.api_base or "https://api.deepseek.com",
            timeout=self.config.timeout
        )
  
    async def generate(
        self, 
        messages: List[Dict[str, str]], 
        **kwargs
    ) -> LLMResponse:
        """ç”Ÿæˆå“åº” - ä¸ OpenAI å…¼å®¹"""
        if not self._client:
            await self.initialize()
      
        response = await self._client.chat.completions.create(
            model=self.config.model_name,
            messages=messages,
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens,
            **kwargs
        )
      
        return LLMResponse(
            content=response.choices[0].message.content,
            model=response.model,
            usage={
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens,
            }
        )
  
    async def stream_generate(
        self, 
        messages: List[Dict[str, str]], 
        **kwargs
    ) -> AsyncIterator[str]:
        """æµå¼ç”Ÿæˆ"""
        if not self._client:
            await self.initialize()
      
        stream = await self._client.chat.completions.create(
            model=self.config.model_name,
            messages=messages,
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens,
            stream=True,
            **kwargs
        )
      
        async for chunk in stream:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
  
    def get_langchain_llm(self):
        """è¿”å› LangChain LLM å®ä¾‹"""
        return ChatOpenAI(
            model=self.config.model_name,
            api_key=self.config.api_key,
            base_url=self.config.api_base or "https://api.deepseek.com",
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens,
        )
  
    async def close(self):
        """å…³é—­è¿æ¥"""
        if self._client:
            await self._client.close()
```

---

### 2. Agent åŸºç¡€æ¶æ„

#### `modules/agent/base/agent_base.py`

```python
from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional
from dataclasses import dataclass
from langchain_core.messages import BaseMessage

from src.shared.infrastructure.llm.base import BaseLLMProvider

@dataclass
class AgentConfig:
    """Agent é…ç½®"""
    name: str
    description: str
    llm_provider: BaseLLMProvider
    system_prompt: str
    tools: List[Any] = None
    memory_enabled: bool = True
    max_iterations: int = 5

class BaseAgent(ABC):
    """Agent åŸºç±»"""
  
    def __init__(self, config: AgentConfig):
        self.config = config
        self.llm = config.llm_provider.get_langchain_llm()
        self.tools = config.tools or []
        self.memory = [] if config.memory_enabled else None
  
    @abstractmethod
    async def execute(self, task: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œä»»åŠ¡"""
        pass
  
    def add_to_memory(self, message: BaseMessage):
        """æ·»åŠ åˆ°è®°å¿†"""
        if self.memory is not None:
            self.memory.append(message)
  
    def get_memory(self) -> List[BaseMessage]:
        """è·å–è®°å¿†"""
        return self.memory or []
  
    def clear_memory(self):
        """æ¸…ç©ºè®°å¿†"""
        if self.memory is not None:
            self.memory.clear()
```

#### `modules/agent/agents/researcher_agent.py`

```python
from typing import Dict, Any
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

from ..base.agent_base import BaseAgent

class ResearcherAgent(BaseAgent):
    """ç ”ç©¶å‘˜ Agent - è´Ÿè´£ä¿¡æ¯æ”¶é›†å’Œç ”ç©¶"""
  
    def __init__(self, config):
        super().__init__(config)
      
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", self.config.system_prompt),
            ("user", "{task}\n\nContext: {context}")
        ])
      
        self.chain = self.prompt | self.llm | StrOutputParser()
  
    async def execute(self, task: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œç ”ç©¶ä»»åŠ¡"""
        try:
            # æ‰§è¡Œç ”ç©¶
            result = await self.chain.ainvoke({
                "task": task,
                "context": context
            })
          
            return {
                "agent": self.config.name,
                "status": "success",
                "result": result,
                "metadata": {
                    "task": task,
                    "context": context
                }
            }
        except Exception as e:
            return {
                "agent": self.config.name,
                "status": "error",
                "error": str(e)
            }
```

---

### 3. LangGraph å›¢é˜Ÿç¼–æ’

#### `modules/agent/teams/graph_builder.py`

```python
from typing import Dict, Any, List, Annotated
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolExecutor
from langchain_core.messages import HumanMessage
import operator

from ..agents.researcher_agent import ResearcherAgent
from ..agents.planner_agent import PlannerAgent
from ..agents.code_generator_agent import CodeGeneratorAgent
from ..agents.reviewer_agent import ReviewerAgent
from ..agents.executor_agent import ExecutorAgent
from ..agents.qa_agent import QAAgent
from ..agents.reporter_agent import ReporterAgent

class AgentState:
    """Agent å›¢é˜ŸçŠ¶æ€"""
    messages: Annotated[List[HumanMessage], operator.add]
    current_task: str
    research_results: Dict[str, Any]
    plan: Dict[str, Any]
    code: str
    review_feedback: str
    execution_result: Dict[str, Any]
    qa_report: Dict[str, Any]
    final_report: str
    next_agent: str

class AgentTeamGraph:
    """7-Agent å›¢é˜Ÿå›¾æ„å»ºå™¨"""
  
    def __init__(
        self,
        researcher: ResearcherAgent,
        planner: PlannerAgent,
        code_generator: CodeGeneratorAgent,
        reviewer: ReviewerAgent,
        executor: ExecutorAgent,
        qa: QAAgent,
        reporter: ReporterAgent
    ):
        self.agents = {
            "researcher": researcher,
            "planner": planner,
            "code_generator": code_generator,
            "reviewer": reviewer,
            "executor": executor,
            "qa": qa,
            "reporter": reporter
        }
      
        self.graph = self._build_graph()
  
    def _build_graph(self) -> StateGraph:
        """æ„å»ºå·¥ä½œæµå›¾"""
        workflow = StateGraph(AgentState)
      
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("researcher", self._researcher_node)
        workflow.add_node("planner", self._planner_node)
        workflow.add_node("code_generator", self._code_generator_node)
        workflow.add_node("reviewer", self._reviewer_node)
        workflow.add_node("executor", self._executor_node)
        workflow.add_node("qa", self._qa_node)
        workflow.add_node("reporter", self._reporter_node)
      
        # å®šä¹‰è¾¹
        workflow.set_entry_point("researcher")
        workflow.add_edge("researcher", "planner")
        workflow.add_edge("planner", "code_generator")
        workflow.add_edge("code_generator", "reviewer")
      
        # æ¡ä»¶è¾¹ï¼šå®¡æ ¸é€šè¿‡ -> æ‰§è¡Œï¼Œå¦åˆ™ -> é‡æ–°ç”Ÿæˆ
        workflow.add_conditional_edges(
            "reviewer",
            self._should_regenerate,
            {
                "regenerate": "code_generator",
                "execute": "executor"
            }
        )
      
        workflow.add_edge("executor", "qa")
      
        # æ¡ä»¶è¾¹ï¼šQA é€šè¿‡ -> æŠ¥å‘Šï¼Œå¦åˆ™ -> é‡æ–°æ‰§è¡Œ
        workflow.add_conditional_edges(
            "qa",
            self._should_reexecute,
            {
                "reexecute": "executor",
                "report": "reporter"
            }
        )
      
        workflow.add_edge("reporter", END)
      
        return workflow.compile()
  
    async def _researcher_node(self, state: AgentState) -> AgentState:
        """ç ”ç©¶å‘˜èŠ‚ç‚¹"""
        result = await self.agents["researcher"].execute(
            state["current_task"], 
            {}
        )
        state["research_results"] = result
        return state
  
    async def _planner_node(self, state: AgentState) -> AgentState:
        """è§„åˆ’å¸ˆèŠ‚ç‚¹"""
        result = await self.agents["planner"].execute(
            state["current_task"],
            {"research": state["research_results"]}
        )
        state["plan"] = result
        return state
  
    async def _code_generator_node(self, state: AgentState) -> AgentState:
        """ä»£ç ç”Ÿæˆå™¨èŠ‚ç‚¹"""
        result = await self.agents["code_generator"].execute(
            state["current_task"],
            {
                "plan": state["plan"],
                "review_feedback": state.get("review_feedback")
            }
        )
        state["code"] = result["result"]
        return state
  
    async def _reviewer_node(self, state: AgentState) -> AgentState:
        """å®¡æ ¸å‘˜èŠ‚ç‚¹"""
        result = await self.agents["reviewer"].execute(
            "Review the generated code",
            {"code": state["code"]}
        )
        state["review_feedback"] = result["result"]
        return state
  
    def _should_regenerate(self, state: AgentState) -> str:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦é‡æ–°ç”Ÿæˆä»£ç """
        feedback = state.get("review_feedback", "")
        if "approved" in feedback.lower():
            return "execute"
        return "regenerate"
  
    async def _executor_node(self, state: AgentState) -> AgentState:
        """æ‰§è¡Œå™¨èŠ‚ç‚¹"""
        result = await self.agents["executor"].execute(
            "Execute the code",
            {"code": state["code"]}
        )
        state["execution_result"] = result
        return state
  
    async def _qa_node(self, state: AgentState) -> AgentState:
        """è´¨æ£€å‘˜èŠ‚ç‚¹"""
        result = await self.agents["qa"].execute(
            "Quality check",
            {"execution_result": state["execution_result"]}
        )
        state["qa_report"] = result
        return state
  
    def _should_reexecute(self, state: AgentState) -> str:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦é‡æ–°æ‰§è¡Œ"""
        qa_report = state.get("qa_report", {})
        if qa_report.get("status") == "passed":
            return "report"
        return "reexecute"
  
    async def _reporter_node(self, state: AgentState) -> AgentState:
        """æŠ¥å‘Šå‘˜èŠ‚ç‚¹"""
        result = await self.agents["reporter"].execute(
            "Generate final report",
            {
                "research": state["research_results"],
                "plan": state["plan"],
                "code": state["code"],
                "execution": state["execution_result"],
                "qa": state["qa_report"]
            }
        )
        state["final_report"] = result["result"]
        return state
  
    async def run(self, task: str) -> Dict[str, Any]:
        """è¿è¡Œå›¢é˜Ÿå·¥ä½œæµ"""
        initial_state = AgentState(
            messages=[HumanMessage(content=task)],
            current_task=task,
            research_results={},
            plan={},
            code="",
            review_feedback="",
            execution_result={},
            qa_report={},
            final_report="",
            next_agent="researcher"
        )
      
        result = await self.graph.ainvoke(initial_state)
        return result
```

---

### 4. API é›†æˆ

#### `modules/agent/api/routes.py`

```python
from fastapi import APIRouter, Depends, HTTPException
from typing import Dict, Any

from .schemas import TaskRequest, TaskResponse
from .dependencies import get_agent_team
from ..teams.graph_builder import AgentTeamGraph

router = APIRouter(prefix="/api/v1/agent", tags=["agent"])

@router.post("/execute", response_model=TaskResponse)
async def execute_task(
    request: TaskRequest,
    agent_team: AgentTeamGraph = Depends(get_agent_team)
) -> TaskResponse:
    """æ‰§è¡Œ Agent å›¢é˜Ÿä»»åŠ¡"""
    try:
        result = await agent_team.run(request.task)
      
        return TaskResponse(
            task_id=request.task_id,
            status="completed",
            result=result["final_report"],
            metadata={
                "research": result["research_results"],
                "plan": result["plan"],
                "qa_report": result["qa_report"]
            }
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy"}
```

#### `modules/agent/api/dependencies.py`

```python
from functools import lru_cache
from typing import Annotated
from fastapi import Depends

from src.shared.infrastructure.llm.factory import LLMFactory
from src.shared.infrastructure.llm.base import LLMConfig, LLMProvider
from ..teams.graph_builder import AgentTeamGraph
from ..agents.researcher_agent import ResearcherAgent
from ..agents.planner_agent import PlannerAgent
# ... å…¶ä»– Agent imports

from ..base.agent_base import AgentConfig

@lru_cache()
def get_llm_provider():
    """è·å– LLM æä¾›å•†ï¼ˆå¯é…ç½®ï¼‰"""
    config = LLMConfig(
        provider=LLMProvider.OPENAI,  # å¯ä»ç¯å¢ƒå˜é‡è¯»å–
        model_name="gpt-4",
        api_key="your-api-key",
        temperature=0.7
    )
    return LLMFactory.create(config)

async def get_agent_team(
    llm_provider = Depends(get_llm_provider)
) -> AgentTeamGraph:
    """è·å– Agent å›¢é˜Ÿå®ä¾‹"""
  
    # åˆå§‹åŒ–å„ä¸ª Agent
    researcher = ResearcherAgent(AgentConfig(
        name="researcher",
        description="Research and gather information",
        llm_provider=llm_provider,
        system_prompt="You are a research agent..."
    ))
  
    planner = PlannerAgent(AgentConfig(
        name="planner",
        description="Create execution plans",
        llm_provider=llm_provider,
        system_prompt="You are a planning agent..."
    ))
  
    # ... åˆå§‹åŒ–å…¶ä»– 5 ä¸ª Agent
  
    return AgentTeamGraph(
        researcher=researcher,
        planner=planner,
        code_generator=code_generator,
        reviewer=reviewer,
        executor=executor,
        qa=qa,
        reporter=reporter
    )
```

---

### 5. é…ç½®ç®¡ç†

#### `config/ai.py`

```python
from pydantic_settings import BaseSettings
from typing import Dict, Any

class AISettings(BaseSettings):
    """AI é…ç½®"""
  
    # é»˜è®¤ LLM æä¾›å•†
    DEFAULT_LLM_PROVIDER: str = "openai"
    DEFAULT_LLM_MODEL: str = "gpt-4"
  
    # OpenAI
    OPENAI_API_KEY: str = ""
    OPENAI_BASE_URL: str = "https://api.openai.com/v1"
  
    # Anthropic
    ANTHROPIC_API_KEY: str = ""
  
    # Azure
    AZURE_OPENAI_API_KEY: str = ""
    AZURE_OPENAI_ENDPOINT: str = ""
  
    # Google
    GOOGLE_API_KEY: str = ""
  
    # DeepSeek
    DEEPSEEK_API_KEY: str = ""
    DEEPSEEK_BASE_URL: str = "https://api.deepseek.com"
  
    # Local (Ollama)
    LOCAL_BASE_URL: str = "http://localhost:11434"
    LOCAL_MODEL: str = "llama2"
  
    # Agent é…ç½®
    AGENT_MAX_ITERATIONS: int = 5
    AGENT_TIMEOUT: int = 300
  
    class Config:
        env_file = ".env"
        case_sensitive = True

ai_settings = AISettings()
```

---

## ğŸ”§ ä½¿ç”¨ç¤ºä¾‹

### 1. åˆå§‹åŒ–ä¸åŒ LLM æä¾›å•†

```python
from src.shared.infrastructure.llm.factory import LLMFactory
from src.shared.infrastructure.llm.base import LLMConfig, LLMProvider

# OpenAI
openai_config = LLMConfig(
    provider=LLMProvider.OPENAI,
    model_name="gpt-4",
    api_key="sk-..."
)
openai_llm = LLMFactory.create(openai_config)

# DeepSeek
deepseek_config = LLMConfig(
    provider=LLMProvider.DEEPSEEK,
    model_name="deepseek-chat",
    api_key="sk-..."
)
deepseek_llm = LLMFactory.create(deepseek_config)

# Local Ollama
local_config = LLMConfig(
    provider=LLMProvider.LOCAL,
    model_name="llama2",
    api_base="http://localhost:11434"
)
local_llm = LLMFactory.create(local_config)
```

### 2. è¿è¡Œ Agent å›¢é˜Ÿ

```python
# API è°ƒç”¨
import httpx

response = await httpx.post(
    "http://localhost:8000/api/v1/agent/execute",
    json={
        "task_id": "task-123",
        "task": "Create a Python web scraper for news articles"
    }
)

result = response.json()
print(result["result"])  # æœ€ç»ˆæŠ¥å‘Š
```

---

## ğŸ“Š ä¼˜åŠ¿æ€»ç»“

1. **æ¨¡å—åŒ–è®¾è®¡**ï¼šå„ç»„ä»¶èŒè´£æ¸…æ™°ï¼Œæ˜“äºç»´æŠ¤
2. **æä¾›å•†æŠ½è±¡**ï¼šè½»æ¾åˆ‡æ¢ä¸åŒ LLM ä¾›åº”å•†
3. **å¯æ‰©å±•æ€§**ï¼šæ–¹ä¾¿æ·»åŠ æ–°çš„ Agent æˆ–å·¥å…·
4. **LangGraph ç¼–æ’**ï¼šå¼ºå¤§çš„å·¥ä½œæµæ§åˆ¶èƒ½åŠ›
5. **ç±»å‹å®‰å…¨**ï¼šå®Œæ•´çš„ç±»å‹æç¤º
6. **æµ‹è¯•å‹å¥½**ï¼šä¾èµ–æ³¨å…¥ä¾¿äºå•å…ƒæµ‹è¯•

è¿™ä¸ªæ¶æ„å¯ä»¥æ”¯æ’‘å¤æ‚çš„ Agent åä½œåœºæ™¯ï¼ŒåŒæ—¶ä¿æŒä»£ç æ•´æ´å’Œå¯ç»´æŠ¤æ€§ï¼ğŸš€